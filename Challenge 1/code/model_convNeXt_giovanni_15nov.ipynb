{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN95jKHa8QbANGzKP587+J+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yCpJzPgXUCRe","outputId":"fc52d497-20b2-4a0d-8c05-3389baf80d03","executionInfo":{"status":"ok","timestamp":1700060736335,"user_tz":-60,"elapsed":4835683,"user":{"displayName":"GiOdeZ #218","userId":"07499849243134024453"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","/gdrive/My Drive/Colab Notebooks/AN2DL\n","Finished loading libraries\n","[*] Training model  model_ConNeXt_large_2_ridge_5 ...\n","Len of positions_to_remove:  196\n","\n","\n","SHAPES OF THE SETS:\n","\n","images_train shape: (4253, 96, 96, 3), labels_train shape: (4253, 2)\n","images_val shape: (676, 96, 96, 3), labels_val shape: (676, 2)\n","images_test shape: (75, 96, 96, 3), labels_test shape: (75, 2)\n","\n","\n","\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/convnext/convnext_large_notop.h5\n","785596384/785596384 [==============================] - 3s 0us/step\n","[*] Network name:  convnext_large\n","0 input_1 True\n","1 convnext_large_prestem_normalization True\n","2 convnext_large_stem True\n","3 convnext_large_stage_0_block_0_depthwise_conv True\n","4 convnext_large_stage_0_block_0_layernorm True\n","5 convnext_large_stage_0_block_0_pointwise_conv_1 True\n","6 convnext_large_stage_0_block_0_gelu True\n","7 convnext_large_stage_0_block_0_pointwise_conv_2 True\n","8 convnext_large_stage_0_block_0_layer_scale True\n","9 convnext_large_stage_0_block_0_identity True\n","10 tf.__operators__.add True\n","11 convnext_large_stage_0_block_1_depthwise_conv True\n","12 convnext_large_stage_0_block_1_layernorm True\n","13 convnext_large_stage_0_block_1_pointwise_conv_1 True\n","14 convnext_large_stage_0_block_1_gelu True\n","15 convnext_large_stage_0_block_1_pointwise_conv_2 True\n","16 convnext_large_stage_0_block_1_layer_scale True\n","17 convnext_large_stage_0_block_1_identity True\n","18 tf.__operators__.add_1 True\n","19 convnext_large_stage_0_block_2_depthwise_conv True\n","20 convnext_large_stage_0_block_2_layernorm True\n","21 convnext_large_stage_0_block_2_pointwise_conv_1 True\n","22 convnext_large_stage_0_block_2_gelu True\n","23 convnext_large_stage_0_block_2_pointwise_conv_2 True\n","24 convnext_large_stage_0_block_2_layer_scale True\n","25 convnext_large_stage_0_block_2_identity True\n","26 tf.__operators__.add_2 True\n","27 convnext_large_downsampling_block_0 True\n","28 convnext_large_stage_1_block_0_depthwise_conv True\n","29 convnext_large_stage_1_block_0_layernorm True\n","30 convnext_large_stage_1_block_0_pointwise_conv_1 True\n","31 convnext_large_stage_1_block_0_gelu True\n","32 convnext_large_stage_1_block_0_pointwise_conv_2 True\n","33 convnext_large_stage_1_block_0_layer_scale True\n","34 convnext_large_stage_1_block_0_identity True\n","35 tf.__operators__.add_3 True\n","36 convnext_large_stage_1_block_1_depthwise_conv True\n","37 convnext_large_stage_1_block_1_layernorm True\n","38 convnext_large_stage_1_block_1_pointwise_conv_1 True\n","39 convnext_large_stage_1_block_1_gelu True\n","40 convnext_large_stage_1_block_1_pointwise_conv_2 True\n","41 convnext_large_stage_1_block_1_layer_scale True\n","42 convnext_large_stage_1_block_1_identity True\n","43 tf.__operators__.add_4 True\n","44 convnext_large_stage_1_block_2_depthwise_conv True\n","45 convnext_large_stage_1_block_2_layernorm True\n","46 convnext_large_stage_1_block_2_pointwise_conv_1 True\n","47 convnext_large_stage_1_block_2_gelu True\n","48 convnext_large_stage_1_block_2_pointwise_conv_2 True\n","49 convnext_large_stage_1_block_2_layer_scale True\n","50 convnext_large_stage_1_block_2_identity True\n","51 tf.__operators__.add_5 True\n","52 convnext_large_downsampling_block_1 True\n","53 convnext_large_stage_2_block_0_depthwise_conv True\n","54 convnext_large_stage_2_block_0_layernorm True\n","55 convnext_large_stage_2_block_0_pointwise_conv_1 True\n","56 convnext_large_stage_2_block_0_gelu True\n","57 convnext_large_stage_2_block_0_pointwise_conv_2 True\n","58 convnext_large_stage_2_block_0_layer_scale True\n","59 convnext_large_stage_2_block_0_identity True\n","60 tf.__operators__.add_6 True\n","61 convnext_large_stage_2_block_1_depthwise_conv True\n","62 convnext_large_stage_2_block_1_layernorm True\n","63 convnext_large_stage_2_block_1_pointwise_conv_1 True\n","64 convnext_large_stage_2_block_1_gelu True\n","65 convnext_large_stage_2_block_1_pointwise_conv_2 True\n","66 convnext_large_stage_2_block_1_layer_scale True\n","67 convnext_large_stage_2_block_1_identity True\n","68 tf.__operators__.add_7 True\n","69 convnext_large_stage_2_block_2_depthwise_conv True\n","70 convnext_large_stage_2_block_2_layernorm True\n","71 convnext_large_stage_2_block_2_pointwise_conv_1 True\n","72 convnext_large_stage_2_block_2_gelu True\n","73 convnext_large_stage_2_block_2_pointwise_conv_2 True\n","74 convnext_large_stage_2_block_2_layer_scale True\n","75 convnext_large_stage_2_block_2_identity True\n","76 tf.__operators__.add_8 True\n","77 convnext_large_stage_2_block_3_depthwise_conv True\n","78 convnext_large_stage_2_block_3_layernorm True\n","79 convnext_large_stage_2_block_3_pointwise_conv_1 True\n","80 convnext_large_stage_2_block_3_gelu True\n","81 convnext_large_stage_2_block_3_pointwise_conv_2 True\n","82 convnext_large_stage_2_block_3_layer_scale True\n","83 convnext_large_stage_2_block_3_identity True\n","84 tf.__operators__.add_9 True\n","85 convnext_large_stage_2_block_4_depthwise_conv True\n","86 convnext_large_stage_2_block_4_layernorm True\n","87 convnext_large_stage_2_block_4_pointwise_conv_1 True\n","88 convnext_large_stage_2_block_4_gelu True\n","89 convnext_large_stage_2_block_4_pointwise_conv_2 True\n","90 convnext_large_stage_2_block_4_layer_scale True\n","91 convnext_large_stage_2_block_4_identity True\n","92 tf.__operators__.add_10 True\n","93 convnext_large_stage_2_block_5_depthwise_conv True\n","94 convnext_large_stage_2_block_5_layernorm True\n","95 convnext_large_stage_2_block_5_pointwise_conv_1 True\n","96 convnext_large_stage_2_block_5_gelu True\n","97 convnext_large_stage_2_block_5_pointwise_conv_2 True\n","98 convnext_large_stage_2_block_5_layer_scale True\n","99 convnext_large_stage_2_block_5_identity True\n","100 tf.__operators__.add_11 True\n","101 convnext_large_stage_2_block_6_depthwise_conv True\n","102 convnext_large_stage_2_block_6_layernorm True\n","103 convnext_large_stage_2_block_6_pointwise_conv_1 True\n","104 convnext_large_stage_2_block_6_gelu True\n","105 convnext_large_stage_2_block_6_pointwise_conv_2 True\n","106 convnext_large_stage_2_block_6_layer_scale True\n","107 convnext_large_stage_2_block_6_identity True\n","108 tf.__operators__.add_12 True\n","109 convnext_large_stage_2_block_7_depthwise_conv True\n","110 convnext_large_stage_2_block_7_layernorm True\n","111 convnext_large_stage_2_block_7_pointwise_conv_1 True\n","112 convnext_large_stage_2_block_7_gelu True\n","113 convnext_large_stage_2_block_7_pointwise_conv_2 True\n","114 convnext_large_stage_2_block_7_layer_scale True\n","115 convnext_large_stage_2_block_7_identity True\n","116 tf.__operators__.add_13 True\n","117 convnext_large_stage_2_block_8_depthwise_conv True\n","118 convnext_large_stage_2_block_8_layernorm True\n","119 convnext_large_stage_2_block_8_pointwise_conv_1 True\n","120 convnext_large_stage_2_block_8_gelu True\n","121 convnext_large_stage_2_block_8_pointwise_conv_2 True\n","122 convnext_large_stage_2_block_8_layer_scale True\n","123 convnext_large_stage_2_block_8_identity True\n","124 tf.__operators__.add_14 True\n","125 convnext_large_stage_2_block_9_depthwise_conv True\n","126 convnext_large_stage_2_block_9_layernorm True\n","127 convnext_large_stage_2_block_9_pointwise_conv_1 True\n","128 convnext_large_stage_2_block_9_gelu True\n","129 convnext_large_stage_2_block_9_pointwise_conv_2 True\n","130 convnext_large_stage_2_block_9_layer_scale True\n","131 convnext_large_stage_2_block_9_identity True\n","132 tf.__operators__.add_15 True\n","133 convnext_large_stage_2_block_10_depthwise_conv True\n","134 convnext_large_stage_2_block_10_layernorm True\n","135 convnext_large_stage_2_block_10_pointwise_conv_1 True\n","136 convnext_large_stage_2_block_10_gelu True\n","137 convnext_large_stage_2_block_10_pointwise_conv_2 True\n","138 convnext_large_stage_2_block_10_layer_scale True\n","139 convnext_large_stage_2_block_10_identity True\n","140 tf.__operators__.add_16 True\n","141 convnext_large_stage_2_block_11_depthwise_conv True\n","142 convnext_large_stage_2_block_11_layernorm True\n","143 convnext_large_stage_2_block_11_pointwise_conv_1 True\n","144 convnext_large_stage_2_block_11_gelu True\n","145 convnext_large_stage_2_block_11_pointwise_conv_2 True\n","146 convnext_large_stage_2_block_11_layer_scale True\n","147 convnext_large_stage_2_block_11_identity True\n","148 tf.__operators__.add_17 True\n","149 convnext_large_stage_2_block_12_depthwise_conv True\n","150 convnext_large_stage_2_block_12_layernorm True\n","151 convnext_large_stage_2_block_12_pointwise_conv_1 True\n","152 convnext_large_stage_2_block_12_gelu True\n","153 convnext_large_stage_2_block_12_pointwise_conv_2 True\n","154 convnext_large_stage_2_block_12_layer_scale True\n","155 convnext_large_stage_2_block_12_identity True\n","156 tf.__operators__.add_18 True\n","157 convnext_large_stage_2_block_13_depthwise_conv True\n","158 convnext_large_stage_2_block_13_layernorm True\n","159 convnext_large_stage_2_block_13_pointwise_conv_1 True\n","160 convnext_large_stage_2_block_13_gelu True\n","161 convnext_large_stage_2_block_13_pointwise_conv_2 True\n","162 convnext_large_stage_2_block_13_layer_scale True\n","163 convnext_large_stage_2_block_13_identity True\n","164 tf.__operators__.add_19 True\n","165 convnext_large_stage_2_block_14_depthwise_conv True\n","166 convnext_large_stage_2_block_14_layernorm True\n","167 convnext_large_stage_2_block_14_pointwise_conv_1 True\n","168 convnext_large_stage_2_block_14_gelu True\n","169 convnext_large_stage_2_block_14_pointwise_conv_2 True\n","170 convnext_large_stage_2_block_14_layer_scale True\n","171 convnext_large_stage_2_block_14_identity True\n","172 tf.__operators__.add_20 True\n","173 convnext_large_stage_2_block_15_depthwise_conv True\n","174 convnext_large_stage_2_block_15_layernorm True\n","175 convnext_large_stage_2_block_15_pointwise_conv_1 True\n","176 convnext_large_stage_2_block_15_gelu True\n","177 convnext_large_stage_2_block_15_pointwise_conv_2 True\n","178 convnext_large_stage_2_block_15_layer_scale True\n","179 convnext_large_stage_2_block_15_identity True\n","180 tf.__operators__.add_21 True\n","181 convnext_large_stage_2_block_16_depthwise_conv True\n","182 convnext_large_stage_2_block_16_layernorm True\n","183 convnext_large_stage_2_block_16_pointwise_conv_1 True\n","184 convnext_large_stage_2_block_16_gelu True\n","185 convnext_large_stage_2_block_16_pointwise_conv_2 True\n","186 convnext_large_stage_2_block_16_layer_scale True\n","187 convnext_large_stage_2_block_16_identity True\n","188 tf.__operators__.add_22 True\n","189 convnext_large_stage_2_block_17_depthwise_conv True\n","190 convnext_large_stage_2_block_17_layernorm True\n","191 convnext_large_stage_2_block_17_pointwise_conv_1 True\n","192 convnext_large_stage_2_block_17_gelu True\n","193 convnext_large_stage_2_block_17_pointwise_conv_2 True\n","194 convnext_large_stage_2_block_17_layer_scale True\n","195 convnext_large_stage_2_block_17_identity True\n","196 tf.__operators__.add_23 True\n","197 convnext_large_stage_2_block_18_depthwise_conv True\n","198 convnext_large_stage_2_block_18_layernorm True\n","199 convnext_large_stage_2_block_18_pointwise_conv_1 True\n","200 convnext_large_stage_2_block_18_gelu True\n","201 convnext_large_stage_2_block_18_pointwise_conv_2 True\n","202 convnext_large_stage_2_block_18_layer_scale True\n","203 convnext_large_stage_2_block_18_identity True\n","204 tf.__operators__.add_24 True\n","205 convnext_large_stage_2_block_19_depthwise_conv True\n","206 convnext_large_stage_2_block_19_layernorm True\n","207 convnext_large_stage_2_block_19_pointwise_conv_1 True\n","208 convnext_large_stage_2_block_19_gelu True\n","209 convnext_large_stage_2_block_19_pointwise_conv_2 True\n","210 convnext_large_stage_2_block_19_layer_scale True\n","211 convnext_large_stage_2_block_19_identity True\n","212 tf.__operators__.add_25 True\n","213 convnext_large_stage_2_block_20_depthwise_conv True\n","214 convnext_large_stage_2_block_20_layernorm True\n","215 convnext_large_stage_2_block_20_pointwise_conv_1 True\n","216 convnext_large_stage_2_block_20_gelu True\n","217 convnext_large_stage_2_block_20_pointwise_conv_2 True\n","218 convnext_large_stage_2_block_20_layer_scale True\n","219 convnext_large_stage_2_block_20_identity True\n","220 tf.__operators__.add_26 True\n","221 convnext_large_stage_2_block_21_depthwise_conv True\n","222 convnext_large_stage_2_block_21_layernorm True\n","223 convnext_large_stage_2_block_21_pointwise_conv_1 True\n","224 convnext_large_stage_2_block_21_gelu True\n","225 convnext_large_stage_2_block_21_pointwise_conv_2 True\n","226 convnext_large_stage_2_block_21_layer_scale True\n","227 convnext_large_stage_2_block_21_identity True\n","228 tf.__operators__.add_27 True\n","229 convnext_large_stage_2_block_22_depthwise_conv True\n","230 convnext_large_stage_2_block_22_layernorm True\n","231 convnext_large_stage_2_block_22_pointwise_conv_1 True\n","232 convnext_large_stage_2_block_22_gelu True\n","233 convnext_large_stage_2_block_22_pointwise_conv_2 True\n","234 convnext_large_stage_2_block_22_layer_scale True\n","235 convnext_large_stage_2_block_22_identity True\n","236 tf.__operators__.add_28 True\n","237 convnext_large_stage_2_block_23_depthwise_conv True\n","238 convnext_large_stage_2_block_23_layernorm True\n","239 convnext_large_stage_2_block_23_pointwise_conv_1 True\n","240 convnext_large_stage_2_block_23_gelu True\n","241 convnext_large_stage_2_block_23_pointwise_conv_2 True\n","242 convnext_large_stage_2_block_23_layer_scale True\n","243 convnext_large_stage_2_block_23_identity True\n","244 tf.__operators__.add_29 True\n","245 convnext_large_stage_2_block_24_depthwise_conv True\n","246 convnext_large_stage_2_block_24_layernorm True\n","247 convnext_large_stage_2_block_24_pointwise_conv_1 True\n","248 convnext_large_stage_2_block_24_gelu True\n","249 convnext_large_stage_2_block_24_pointwise_conv_2 True\n","250 convnext_large_stage_2_block_24_layer_scale True\n","251 convnext_large_stage_2_block_24_identity True\n","252 tf.__operators__.add_30 True\n","253 convnext_large_stage_2_block_25_depthwise_conv True\n","254 convnext_large_stage_2_block_25_layernorm True\n","255 convnext_large_stage_2_block_25_pointwise_conv_1 True\n","256 convnext_large_stage_2_block_25_gelu True\n","257 convnext_large_stage_2_block_25_pointwise_conv_2 True\n","258 convnext_large_stage_2_block_25_layer_scale True\n","259 convnext_large_stage_2_block_25_identity True\n","260 tf.__operators__.add_31 True\n","261 convnext_large_stage_2_block_26_depthwise_conv True\n","262 convnext_large_stage_2_block_26_layernorm True\n","263 convnext_large_stage_2_block_26_pointwise_conv_1 True\n","264 convnext_large_stage_2_block_26_gelu True\n","265 convnext_large_stage_2_block_26_pointwise_conv_2 True\n","266 convnext_large_stage_2_block_26_layer_scale True\n","267 convnext_large_stage_2_block_26_identity True\n","268 tf.__operators__.add_32 True\n","269 convnext_large_downsampling_block_2 True\n","270 convnext_large_stage_3_block_0_depthwise_conv True\n","271 convnext_large_stage_3_block_0_layernorm True\n","272 convnext_large_stage_3_block_0_pointwise_conv_1 True\n","273 convnext_large_stage_3_block_0_gelu True\n","274 convnext_large_stage_3_block_0_pointwise_conv_2 True\n","275 convnext_large_stage_3_block_0_layer_scale True\n","276 convnext_large_stage_3_block_0_identity True\n","277 tf.__operators__.add_33 True\n","278 convnext_large_stage_3_block_1_depthwise_conv True\n","279 convnext_large_stage_3_block_1_layernorm True\n","280 convnext_large_stage_3_block_1_pointwise_conv_1 True\n","281 convnext_large_stage_3_block_1_gelu True\n","282 convnext_large_stage_3_block_1_pointwise_conv_2 True\n","283 convnext_large_stage_3_block_1_layer_scale True\n","284 convnext_large_stage_3_block_1_identity True\n","285 tf.__operators__.add_34 True\n","286 convnext_large_stage_3_block_2_depthwise_conv True\n","287 convnext_large_stage_3_block_2_layernorm True\n","288 convnext_large_stage_3_block_2_pointwise_conv_1 True\n","289 convnext_large_stage_3_block_2_gelu True\n","290 convnext_large_stage_3_block_2_pointwise_conv_2 True\n","291 convnext_large_stage_3_block_2_layer_scale True\n","292 convnext_large_stage_3_block_2_identity True\n","293 tf.__operators__.add_35 True\n","294 global_average_pooling2d True\n","295 layer_normalization True\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 96, 96, 3)]       0         \n","                                                                 \n"," preprocessing (Sequential)  (None, 96, 96, 3)         0         \n","                                                                 \n"," convnext_large (Functional  (None, 1536)              196230336 \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 1536)              0         \n","                                                                 \n"," Output (Dense)              (None, 2)                 3074      \n","                                                                 \n","=================================================================\n","Total params: 196233410 (748.57 MB)\n","Trainable params: 3074 (12.01 KB)\n","Non-trainable params: 196230336 (748.56 MB)\n","_________________________________________________________________\n","Epoch 1/200\n","133/133 [==============================] - 76s 354ms/step - loss: 0.7710 - accuracy: 0.6880 - val_loss: 0.5808 - val_accuracy: 0.7633\n","Epoch 2/200\n","133/133 [==============================] - 34s 253ms/step - loss: 0.5961 - accuracy: 0.7630 - val_loss: 0.4962 - val_accuracy: 0.8033\n","Epoch 3/200\n","133/133 [==============================] - 35s 260ms/step - loss: 0.5378 - accuracy: 0.7766 - val_loss: 0.4658 - val_accuracy: 0.8151\n","Epoch 4/200\n","133/133 [==============================] - 33s 251ms/step - loss: 0.5044 - accuracy: 0.8001 - val_loss: 0.5058 - val_accuracy: 0.7840\n","Epoch 5/200\n","133/133 [==============================] - 33s 248ms/step - loss: 0.4949 - accuracy: 0.7954 - val_loss: 0.4423 - val_accuracy: 0.8092\n","Epoch 6/200\n","133/133 [==============================] - 34s 256ms/step - loss: 0.4844 - accuracy: 0.8018 - val_loss: 0.4106 - val_accuracy: 0.8417\n","Epoch 7/200\n","133/133 [==============================] - 34s 259ms/step - loss: 0.4688 - accuracy: 0.8081 - val_loss: 0.6008 - val_accuracy: 0.7352\n","Epoch 8/200\n","133/133 [==============================] - 34s 253ms/step - loss: 0.4695 - accuracy: 0.8067 - val_loss: 0.4390 - val_accuracy: 0.8062\n","Epoch 9/200\n","133/133 [==============================] - 35s 262ms/step - loss: 0.4679 - accuracy: 0.8055 - val_loss: 0.5090 - val_accuracy: 0.7663\n","Epoch 10/200\n","133/133 [==============================] - 34s 260ms/step - loss: 0.4591 - accuracy: 0.8103 - val_loss: 0.3864 - val_accuracy: 0.8536\n","Epoch 11/200\n","133/133 [==============================] - 34s 258ms/step - loss: 0.4631 - accuracy: 0.8110 - val_loss: 0.4012 - val_accuracy: 0.8388\n","Epoch 12/200\n","133/133 [==============================] - 34s 259ms/step - loss: 0.4587 - accuracy: 0.8091 - val_loss: 0.4265 - val_accuracy: 0.8462\n","Epoch 13/200\n","133/133 [==============================] - 34s 255ms/step - loss: 0.4587 - accuracy: 0.8142 - val_loss: 0.3929 - val_accuracy: 0.8417\n","Epoch 14/200\n","133/133 [==============================] - 35s 260ms/step - loss: 0.4580 - accuracy: 0.8077 - val_loss: 0.4157 - val_accuracy: 0.8210\n","Epoch 15/200\n","133/133 [==============================] - 34s 259ms/step - loss: 0.4505 - accuracy: 0.8145 - val_loss: 0.4040 - val_accuracy: 0.8432\n","Epoch 16/200\n","133/133 [==============================] - 34s 256ms/step - loss: 0.4561 - accuracy: 0.8067 - val_loss: 0.4163 - val_accuracy: 0.8299\n","Epoch 17/200\n","133/133 [==============================] - 34s 259ms/step - loss: 0.4510 - accuracy: 0.8128 - val_loss: 0.3991 - val_accuracy: 0.8358\n","Epoch 18/200\n","133/133 [==============================] - 34s 258ms/step - loss: 0.4634 - accuracy: 0.8023 - val_loss: 0.4252 - val_accuracy: 0.8240\n","Epoch 19/200\n","133/133 [==============================] - 34s 259ms/step - loss: 0.4572 - accuracy: 0.8077 - val_loss: 0.4546 - val_accuracy: 0.7959\n","Epoch 20/200\n","133/133 [==============================] - 34s 260ms/step - loss: 0.4488 - accuracy: 0.8070 - val_loss: 0.4119 - val_accuracy: 0.8388\n","Epoch 21/200\n","133/133 [==============================] - 35s 266ms/step - loss: 0.4618 - accuracy: 0.7926 - val_loss: 0.3865 - val_accuracy: 0.8595\n","Epoch 22/200\n","133/133 [==============================] - 34s 259ms/step - loss: 0.4460 - accuracy: 0.8126 - val_loss: 0.4242 - val_accuracy: 0.8195\n","Epoch 23/200\n","133/133 [==============================] - 34s 256ms/step - loss: 0.4596 - accuracy: 0.8185 - val_loss: 0.3953 - val_accuracy: 0.8417\n","Epoch 24/200\n","133/133 [==============================] - 34s 254ms/step - loss: 0.4454 - accuracy: 0.8138 - val_loss: 0.4052 - val_accuracy: 0.8388\n","Epoch 25/200\n","133/133 [==============================] - 34s 260ms/step - loss: 0.4482 - accuracy: 0.8171 - val_loss: 0.4269 - val_accuracy: 0.8092\n","Epoch 26/200\n","133/133 [==============================] - 34s 259ms/step - loss: 0.4565 - accuracy: 0.8004 - val_loss: 0.3851 - val_accuracy: 0.8506\n","Epoch 27/200\n","133/133 [==============================] - 35s 260ms/step - loss: 0.4506 - accuracy: 0.8070 - val_loss: 0.4159 - val_accuracy: 0.8225\n","Epoch 28/200\n","133/133 [==============================] - 34s 255ms/step - loss: 0.4642 - accuracy: 0.8070 - val_loss: 0.4381 - val_accuracy: 0.8062\n","Epoch 29/200\n","133/133 [==============================] - 34s 255ms/step - loss: 0.4564 - accuracy: 0.8093 - val_loss: 0.4045 - val_accuracy: 0.8343\n","Epoch 30/200\n","133/133 [==============================] - 35s 260ms/step - loss: 0.4559 - accuracy: 0.8079 - val_loss: 0.3931 - val_accuracy: 0.8447\n","Epoch 31/200\n","133/133 [==============================] - 34s 260ms/step - loss: 0.4552 - accuracy: 0.8126 - val_loss: 0.3993 - val_accuracy: 0.8388\n","Epoch 32/200\n","133/133 [==============================] - 34s 259ms/step - loss: 0.4616 - accuracy: 0.7999 - val_loss: 0.4057 - val_accuracy: 0.8269\n","Epoch 33/200\n","133/133 [==============================] - 34s 254ms/step - loss: 0.4587 - accuracy: 0.8060 - val_loss: 0.4195 - val_accuracy: 0.8476\n","Epoch 34/200\n","133/133 [==============================] - 35s 260ms/step - loss: 0.4601 - accuracy: 0.8020 - val_loss: 0.4088 - val_accuracy: 0.8328\n","Epoch 35/200\n","133/133 [==============================] - 34s 258ms/step - loss: 0.4668 - accuracy: 0.8060 - val_loss: 0.4053 - val_accuracy: 0.8299\n","Epoch 36/200\n","133/133 [==============================] - 34s 255ms/step - loss: 0.4482 - accuracy: 0.8131 - val_loss: 0.3891 - val_accuracy: 0.8299\n","Epoch 37/200\n","133/133 [==============================] - 35s 260ms/step - loss: 0.4546 - accuracy: 0.7999 - val_loss: 0.3988 - val_accuracy: 0.8550\n","Epoch 38/200\n","133/133 [==============================] - 35s 260ms/step - loss: 0.4313 - accuracy: 0.8211 - val_loss: 0.4317 - val_accuracy: 0.8136\n","Epoch 39/200\n","133/133 [==============================] - 35s 260ms/step - loss: 0.4440 - accuracy: 0.8185 - val_loss: 0.4035 - val_accuracy: 0.8284\n","Epoch 40/200\n","133/133 [==============================] - 35s 260ms/step - loss: 0.4549 - accuracy: 0.8041 - val_loss: 0.4113 - val_accuracy: 0.8373\n","Epoch 41/200\n","133/133 [==============================] - 35s 263ms/step - loss: 0.4500 - accuracy: 0.8081 - val_loss: 0.4968 - val_accuracy: 0.7633\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 96, 96, 3)]       0         \n","                                                                 \n"," preprocessing (Sequential)  (None, 96, 96, 3)         0         \n","                                                                 \n"," convnext_large (Functional  (None, 1536)              196230336 \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 1536)              0         \n","                                                                 \n"," Output (Dense)              (None, 2)                 3074      \n","                                                                 \n","=================================================================\n","Total params: 196233410 (748.57 MB)\n","Trainable params: 3074 (12.01 KB)\n","Non-trainable params: 196230336 (748.56 MB)\n","_________________________________________________________________\n","0 input_1 False\n","1 convnext_large_prestem_normalization False\n","2 convnext_large_stem False\n","3 convnext_large_stage_0_block_0_depthwise_conv False\n","4 convnext_large_stage_0_block_0_layernorm False\n","5 convnext_large_stage_0_block_0_pointwise_conv_1 False\n","6 convnext_large_stage_0_block_0_gelu False\n","7 convnext_large_stage_0_block_0_pointwise_conv_2 False\n","8 convnext_large_stage_0_block_0_layer_scale False\n","9 convnext_large_stage_0_block_0_identity False\n","10 tf.__operators__.add False\n","11 convnext_large_stage_0_block_1_depthwise_conv False\n","12 convnext_large_stage_0_block_1_layernorm False\n","13 convnext_large_stage_0_block_1_pointwise_conv_1 False\n","14 convnext_large_stage_0_block_1_gelu False\n","15 convnext_large_stage_0_block_1_pointwise_conv_2 False\n","16 convnext_large_stage_0_block_1_layer_scale False\n","17 convnext_large_stage_0_block_1_identity False\n","18 tf.__operators__.add_1 False\n","19 convnext_large_stage_0_block_2_depthwise_conv False\n","20 convnext_large_stage_0_block_2_layernorm False\n","21 convnext_large_stage_0_block_2_pointwise_conv_1 False\n","22 convnext_large_stage_0_block_2_gelu False\n","23 convnext_large_stage_0_block_2_pointwise_conv_2 False\n","24 convnext_large_stage_0_block_2_layer_scale False\n","25 convnext_large_stage_0_block_2_identity False\n","26 tf.__operators__.add_2 False\n","27 convnext_large_downsampling_block_0 False\n","28 convnext_large_stage_1_block_0_depthwise_conv False\n","29 convnext_large_stage_1_block_0_layernorm False\n","30 convnext_large_stage_1_block_0_pointwise_conv_1 False\n","31 convnext_large_stage_1_block_0_gelu False\n","32 convnext_large_stage_1_block_0_pointwise_conv_2 False\n","33 convnext_large_stage_1_block_0_layer_scale False\n","34 convnext_large_stage_1_block_0_identity False\n","35 tf.__operators__.add_3 False\n","36 convnext_large_stage_1_block_1_depthwise_conv False\n","37 convnext_large_stage_1_block_1_layernorm False\n","38 convnext_large_stage_1_block_1_pointwise_conv_1 False\n","39 convnext_large_stage_1_block_1_gelu False\n","40 convnext_large_stage_1_block_1_pointwise_conv_2 False\n","41 convnext_large_stage_1_block_1_layer_scale False\n","42 convnext_large_stage_1_block_1_identity False\n","43 tf.__operators__.add_4 False\n","44 convnext_large_stage_1_block_2_depthwise_conv False\n","45 convnext_large_stage_1_block_2_layernorm False\n","46 convnext_large_stage_1_block_2_pointwise_conv_1 False\n","47 convnext_large_stage_1_block_2_gelu False\n","48 convnext_large_stage_1_block_2_pointwise_conv_2 False\n","49 convnext_large_stage_1_block_2_layer_scale False\n","50 convnext_large_stage_1_block_2_identity False\n","51 tf.__operators__.add_5 False\n","52 convnext_large_downsampling_block_1 False\n","53 convnext_large_stage_2_block_0_depthwise_conv False\n","54 convnext_large_stage_2_block_0_layernorm False\n","55 convnext_large_stage_2_block_0_pointwise_conv_1 False\n","56 convnext_large_stage_2_block_0_gelu False\n","57 convnext_large_stage_2_block_0_pointwise_conv_2 False\n","58 convnext_large_stage_2_block_0_layer_scale False\n","59 convnext_large_stage_2_block_0_identity False\n","60 tf.__operators__.add_6 False\n","61 convnext_large_stage_2_block_1_depthwise_conv False\n","62 convnext_large_stage_2_block_1_layernorm False\n","63 convnext_large_stage_2_block_1_pointwise_conv_1 False\n","64 convnext_large_stage_2_block_1_gelu False\n","65 convnext_large_stage_2_block_1_pointwise_conv_2 False\n","66 convnext_large_stage_2_block_1_layer_scale False\n","67 convnext_large_stage_2_block_1_identity False\n","68 tf.__operators__.add_7 False\n","69 convnext_large_stage_2_block_2_depthwise_conv False\n","70 convnext_large_stage_2_block_2_layernorm False\n","71 convnext_large_stage_2_block_2_pointwise_conv_1 False\n","72 convnext_large_stage_2_block_2_gelu False\n","73 convnext_large_stage_2_block_2_pointwise_conv_2 False\n","74 convnext_large_stage_2_block_2_layer_scale False\n","75 convnext_large_stage_2_block_2_identity False\n","76 tf.__operators__.add_8 False\n","77 convnext_large_stage_2_block_3_depthwise_conv False\n","78 convnext_large_stage_2_block_3_layernorm False\n","79 convnext_large_stage_2_block_3_pointwise_conv_1 False\n","80 convnext_large_stage_2_block_3_gelu False\n","81 convnext_large_stage_2_block_3_pointwise_conv_2 False\n","82 convnext_large_stage_2_block_3_layer_scale False\n","83 convnext_large_stage_2_block_3_identity False\n","84 tf.__operators__.add_9 False\n","85 convnext_large_stage_2_block_4_depthwise_conv False\n","86 convnext_large_stage_2_block_4_layernorm False\n","87 convnext_large_stage_2_block_4_pointwise_conv_1 False\n","88 convnext_large_stage_2_block_4_gelu False\n","89 convnext_large_stage_2_block_4_pointwise_conv_2 False\n","90 convnext_large_stage_2_block_4_layer_scale False\n","91 convnext_large_stage_2_block_4_identity False\n","92 tf.__operators__.add_10 False\n","93 convnext_large_stage_2_block_5_depthwise_conv False\n","94 convnext_large_stage_2_block_5_layernorm False\n","95 convnext_large_stage_2_block_5_pointwise_conv_1 False\n","96 convnext_large_stage_2_block_5_gelu False\n","97 convnext_large_stage_2_block_5_pointwise_conv_2 False\n","98 convnext_large_stage_2_block_5_layer_scale False\n","99 convnext_large_stage_2_block_5_identity False\n","100 tf.__operators__.add_11 False\n","101 convnext_large_stage_2_block_6_depthwise_conv False\n","102 convnext_large_stage_2_block_6_layernorm False\n","103 convnext_large_stage_2_block_6_pointwise_conv_1 False\n","104 convnext_large_stage_2_block_6_gelu False\n","105 convnext_large_stage_2_block_6_pointwise_conv_2 False\n","106 convnext_large_stage_2_block_6_layer_scale False\n","107 convnext_large_stage_2_block_6_identity False\n","108 tf.__operators__.add_12 False\n","109 convnext_large_stage_2_block_7_depthwise_conv False\n","110 convnext_large_stage_2_block_7_layernorm False\n","111 convnext_large_stage_2_block_7_pointwise_conv_1 False\n","112 convnext_large_stage_2_block_7_gelu False\n","113 convnext_large_stage_2_block_7_pointwise_conv_2 False\n","114 convnext_large_stage_2_block_7_layer_scale False\n","115 convnext_large_stage_2_block_7_identity False\n","116 tf.__operators__.add_13 False\n","117 convnext_large_stage_2_block_8_depthwise_conv False\n","118 convnext_large_stage_2_block_8_layernorm False\n","119 convnext_large_stage_2_block_8_pointwise_conv_1 False\n","120 convnext_large_stage_2_block_8_gelu False\n","121 convnext_large_stage_2_block_8_pointwise_conv_2 False\n","122 convnext_large_stage_2_block_8_layer_scale False\n","123 convnext_large_stage_2_block_8_identity False\n","124 tf.__operators__.add_14 False\n","125 convnext_large_stage_2_block_9_depthwise_conv True\n","126 convnext_large_stage_2_block_9_layernorm True\n","127 convnext_large_stage_2_block_9_pointwise_conv_1 True\n","128 convnext_large_stage_2_block_9_gelu True\n","129 convnext_large_stage_2_block_9_pointwise_conv_2 True\n","130 convnext_large_stage_2_block_9_layer_scale True\n","131 convnext_large_stage_2_block_9_identity True\n","132 tf.__operators__.add_15 True\n","133 convnext_large_stage_2_block_10_depthwise_conv True\n","134 convnext_large_stage_2_block_10_layernorm True\n","135 convnext_large_stage_2_block_10_pointwise_conv_1 True\n","136 convnext_large_stage_2_block_10_gelu True\n","137 convnext_large_stage_2_block_10_pointwise_conv_2 True\n","138 convnext_large_stage_2_block_10_layer_scale True\n","139 convnext_large_stage_2_block_10_identity True\n","140 tf.__operators__.add_16 True\n","141 convnext_large_stage_2_block_11_depthwise_conv True\n","142 convnext_large_stage_2_block_11_layernorm True\n","143 convnext_large_stage_2_block_11_pointwise_conv_1 True\n","144 convnext_large_stage_2_block_11_gelu True\n","145 convnext_large_stage_2_block_11_pointwise_conv_2 True\n","146 convnext_large_stage_2_block_11_layer_scale True\n","147 convnext_large_stage_2_block_11_identity True\n","148 tf.__operators__.add_17 True\n","149 convnext_large_stage_2_block_12_depthwise_conv True\n","150 convnext_large_stage_2_block_12_layernorm True\n","151 convnext_large_stage_2_block_12_pointwise_conv_1 True\n","152 convnext_large_stage_2_block_12_gelu True\n","153 convnext_large_stage_2_block_12_pointwise_conv_2 True\n","154 convnext_large_stage_2_block_12_layer_scale True\n","155 convnext_large_stage_2_block_12_identity True\n","156 tf.__operators__.add_18 True\n","157 convnext_large_stage_2_block_13_depthwise_conv True\n","158 convnext_large_stage_2_block_13_layernorm True\n","159 convnext_large_stage_2_block_13_pointwise_conv_1 True\n","160 convnext_large_stage_2_block_13_gelu True\n","161 convnext_large_stage_2_block_13_pointwise_conv_2 True\n","162 convnext_large_stage_2_block_13_layer_scale True\n","163 convnext_large_stage_2_block_13_identity True\n","164 tf.__operators__.add_19 True\n","165 convnext_large_stage_2_block_14_depthwise_conv True\n","166 convnext_large_stage_2_block_14_layernorm True\n","167 convnext_large_stage_2_block_14_pointwise_conv_1 True\n","168 convnext_large_stage_2_block_14_gelu True\n","169 convnext_large_stage_2_block_14_pointwise_conv_2 True\n","170 convnext_large_stage_2_block_14_layer_scale True\n","171 convnext_large_stage_2_block_14_identity True\n","172 tf.__operators__.add_20 True\n","173 convnext_large_stage_2_block_15_depthwise_conv True\n","174 convnext_large_stage_2_block_15_layernorm True\n","175 convnext_large_stage_2_block_15_pointwise_conv_1 True\n","176 convnext_large_stage_2_block_15_gelu True\n","177 convnext_large_stage_2_block_15_pointwise_conv_2 True\n","178 convnext_large_stage_2_block_15_layer_scale True\n","179 convnext_large_stage_2_block_15_identity True\n","180 tf.__operators__.add_21 True\n","181 convnext_large_stage_2_block_16_depthwise_conv True\n","182 convnext_large_stage_2_block_16_layernorm True\n","183 convnext_large_stage_2_block_16_pointwise_conv_1 True\n","184 convnext_large_stage_2_block_16_gelu True\n","185 convnext_large_stage_2_block_16_pointwise_conv_2 True\n","186 convnext_large_stage_2_block_16_layer_scale True\n","187 convnext_large_stage_2_block_16_identity True\n","188 tf.__operators__.add_22 True\n","189 convnext_large_stage_2_block_17_depthwise_conv True\n","190 convnext_large_stage_2_block_17_layernorm True\n","191 convnext_large_stage_2_block_17_pointwise_conv_1 True\n","192 convnext_large_stage_2_block_17_gelu True\n","193 convnext_large_stage_2_block_17_pointwise_conv_2 True\n","194 convnext_large_stage_2_block_17_layer_scale True\n","195 convnext_large_stage_2_block_17_identity True\n","196 tf.__operators__.add_23 True\n","197 convnext_large_stage_2_block_18_depthwise_conv True\n","198 convnext_large_stage_2_block_18_layernorm True\n","199 convnext_large_stage_2_block_18_pointwise_conv_1 True\n","200 convnext_large_stage_2_block_18_gelu True\n","201 convnext_large_stage_2_block_18_pointwise_conv_2 True\n","202 convnext_large_stage_2_block_18_layer_scale True\n","203 convnext_large_stage_2_block_18_identity True\n","204 tf.__operators__.add_24 True\n","205 convnext_large_stage_2_block_19_depthwise_conv True\n","206 convnext_large_stage_2_block_19_layernorm True\n","207 convnext_large_stage_2_block_19_pointwise_conv_1 True\n","208 convnext_large_stage_2_block_19_gelu True\n","209 convnext_large_stage_2_block_19_pointwise_conv_2 True\n","210 convnext_large_stage_2_block_19_layer_scale True\n","211 convnext_large_stage_2_block_19_identity True\n","212 tf.__operators__.add_25 True\n","213 convnext_large_stage_2_block_20_depthwise_conv True\n","214 convnext_large_stage_2_block_20_layernorm True\n","215 convnext_large_stage_2_block_20_pointwise_conv_1 True\n","216 convnext_large_stage_2_block_20_gelu True\n","217 convnext_large_stage_2_block_20_pointwise_conv_2 True\n","218 convnext_large_stage_2_block_20_layer_scale True\n","219 convnext_large_stage_2_block_20_identity True\n","220 tf.__operators__.add_26 True\n","221 convnext_large_stage_2_block_21_depthwise_conv True\n","222 convnext_large_stage_2_block_21_layernorm True\n","223 convnext_large_stage_2_block_21_pointwise_conv_1 True\n","224 convnext_large_stage_2_block_21_gelu True\n","225 convnext_large_stage_2_block_21_pointwise_conv_2 True\n","226 convnext_large_stage_2_block_21_layer_scale True\n","227 convnext_large_stage_2_block_21_identity True\n","228 tf.__operators__.add_27 True\n","229 convnext_large_stage_2_block_22_depthwise_conv True\n","230 convnext_large_stage_2_block_22_layernorm True\n","231 convnext_large_stage_2_block_22_pointwise_conv_1 True\n","232 convnext_large_stage_2_block_22_gelu True\n","233 convnext_large_stage_2_block_22_pointwise_conv_2 True\n","234 convnext_large_stage_2_block_22_layer_scale True\n","235 convnext_large_stage_2_block_22_identity True\n","236 tf.__operators__.add_28 True\n","237 convnext_large_stage_2_block_23_depthwise_conv True\n","238 convnext_large_stage_2_block_23_layernorm True\n","239 convnext_large_stage_2_block_23_pointwise_conv_1 True\n","240 convnext_large_stage_2_block_23_gelu True\n","241 convnext_large_stage_2_block_23_pointwise_conv_2 True\n","242 convnext_large_stage_2_block_23_layer_scale True\n","243 convnext_large_stage_2_block_23_identity True\n","244 tf.__operators__.add_29 True\n","245 convnext_large_stage_2_block_24_depthwise_conv True\n","246 convnext_large_stage_2_block_24_layernorm True\n","247 convnext_large_stage_2_block_24_pointwise_conv_1 True\n","248 convnext_large_stage_2_block_24_gelu True\n","249 convnext_large_stage_2_block_24_pointwise_conv_2 True\n","250 convnext_large_stage_2_block_24_layer_scale True\n","251 convnext_large_stage_2_block_24_identity True\n","252 tf.__operators__.add_30 True\n","253 convnext_large_stage_2_block_25_depthwise_conv True\n","254 convnext_large_stage_2_block_25_layernorm True\n","255 convnext_large_stage_2_block_25_pointwise_conv_1 True\n","256 convnext_large_stage_2_block_25_gelu True\n","257 convnext_large_stage_2_block_25_pointwise_conv_2 True\n","258 convnext_large_stage_2_block_25_layer_scale True\n","259 convnext_large_stage_2_block_25_identity True\n","260 tf.__operators__.add_31 True\n","261 convnext_large_stage_2_block_26_depthwise_conv True\n","262 convnext_large_stage_2_block_26_layernorm True\n","263 convnext_large_stage_2_block_26_pointwise_conv_1 True\n","264 convnext_large_stage_2_block_26_gelu True\n","265 convnext_large_stage_2_block_26_pointwise_conv_2 True\n","266 convnext_large_stage_2_block_26_layer_scale True\n","267 convnext_large_stage_2_block_26_identity True\n","268 tf.__operators__.add_32 True\n","269 convnext_large_downsampling_block_2 True\n","270 convnext_large_stage_3_block_0_depthwise_conv True\n","271 convnext_large_stage_3_block_0_layernorm True\n","272 convnext_large_stage_3_block_0_pointwise_conv_1 True\n","273 convnext_large_stage_3_block_0_gelu True\n","274 convnext_large_stage_3_block_0_pointwise_conv_2 True\n","275 convnext_large_stage_3_block_0_layer_scale True\n","276 convnext_large_stage_3_block_0_identity True\n","277 tf.__operators__.add_33 True\n","278 convnext_large_stage_3_block_1_depthwise_conv True\n","279 convnext_large_stage_3_block_1_layernorm True\n","280 convnext_large_stage_3_block_1_pointwise_conv_1 True\n","281 convnext_large_stage_3_block_1_gelu True\n","282 convnext_large_stage_3_block_1_pointwise_conv_2 True\n","283 convnext_large_stage_3_block_1_layer_scale True\n","284 convnext_large_stage_3_block_1_identity True\n","285 tf.__operators__.add_34 True\n","286 convnext_large_stage_3_block_2_depthwise_conv True\n","287 convnext_large_stage_3_block_2_layernorm True\n","288 convnext_large_stage_3_block_2_pointwise_conv_1 True\n","289 convnext_large_stage_3_block_2_gelu True\n","290 convnext_large_stage_3_block_2_pointwise_conv_2 True\n","291 convnext_large_stage_3_block_2_layer_scale True\n","292 convnext_large_stage_3_block_2_identity True\n","293 tf.__operators__.add_35 True\n","294 global_average_pooling2d True\n","295 layer_normalization True\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 96, 96, 3)]       0         \n","                                                                 \n"," preprocessing (Sequential)  (None, 96, 96, 3)         0         \n","                                                                 \n"," convnext_large (Functional  (None, 1536)              196230336 \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 1536)              0         \n","                                                                 \n"," Output (Dense)              (None, 2)                 3074      \n","                                                                 \n","=================================================================\n","Total params: 196233410 (748.57 MB)\n","Trainable params: 147354626 (562.11 MB)\n","Non-trainable params: 48878784 (186.46 MB)\n","_________________________________________________________________\n","Epoch 1/50\n","133/133 [==============================] - 132s 600ms/step - loss: 0.4272 - accuracy: 0.8288 - val_loss: 0.4050 - val_accuracy: 0.8166\n","Epoch 2/50\n","133/133 [==============================] - 64s 480ms/step - loss: 0.3733 - accuracy: 0.8495 - val_loss: 0.3556 - val_accuracy: 0.8580\n","Epoch 3/50\n","133/133 [==============================] - 66s 497ms/step - loss: 0.3323 - accuracy: 0.8686 - val_loss: 0.3244 - val_accuracy: 0.8831\n","Epoch 4/50\n","133/133 [==============================] - 66s 500ms/step - loss: 0.3086 - accuracy: 0.8897 - val_loss: 0.3001 - val_accuracy: 0.8935\n","Epoch 5/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.2847 - accuracy: 0.8914 - val_loss: 0.3123 - val_accuracy: 0.8802\n","Epoch 6/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.2625 - accuracy: 0.9059 - val_loss: 0.3003 - val_accuracy: 0.8861\n","Epoch 7/50\n","133/133 [==============================] - 66s 499ms/step - loss: 0.2550 - accuracy: 0.9111 - val_loss: 0.2942 - val_accuracy: 0.8994\n","Epoch 8/50\n","133/133 [==============================] - 66s 500ms/step - loss: 0.2370 - accuracy: 0.9198 - val_loss: 0.2701 - val_accuracy: 0.9053\n","Epoch 9/50\n","133/133 [==============================] - 66s 495ms/step - loss: 0.2271 - accuracy: 0.9208 - val_loss: 0.2726 - val_accuracy: 0.8994\n","Epoch 10/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.2101 - accuracy: 0.9306 - val_loss: 0.2851 - val_accuracy: 0.8979\n","Epoch 11/50\n","133/133 [==============================] - 66s 499ms/step - loss: 0.1958 - accuracy: 0.9382 - val_loss: 0.2670 - val_accuracy: 0.9098\n","Epoch 12/50\n","133/133 [==============================] - 66s 496ms/step - loss: 0.1884 - accuracy: 0.9356 - val_loss: 0.2512 - val_accuracy: 0.9172\n","Epoch 13/50\n","133/133 [==============================] - 65s 490ms/step - loss: 0.1747 - accuracy: 0.9407 - val_loss: 0.2473 - val_accuracy: 0.9157\n","Epoch 14/50\n","133/133 [==============================] - 65s 489ms/step - loss: 0.1670 - accuracy: 0.9462 - val_loss: 0.2603 - val_accuracy: 0.9127\n","Epoch 15/50\n","133/133 [==============================] - 65s 490ms/step - loss: 0.1621 - accuracy: 0.9457 - val_loss: 0.2507 - val_accuracy: 0.9142\n","Epoch 16/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.1614 - accuracy: 0.9527 - val_loss: 0.2474 - val_accuracy: 0.9172\n","Epoch 17/50\n","133/133 [==============================] - 65s 490ms/step - loss: 0.1472 - accuracy: 0.9572 - val_loss: 0.2503 - val_accuracy: 0.9157\n","Epoch 18/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.1381 - accuracy: 0.9584 - val_loss: 0.2710 - val_accuracy: 0.9112\n","Epoch 19/50\n","133/133 [==============================] - 66s 493ms/step - loss: 0.1348 - accuracy: 0.9603 - val_loss: 0.2584 - val_accuracy: 0.9172\n","Epoch 20/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.1206 - accuracy: 0.9657 - val_loss: 0.2562 - val_accuracy: 0.9157\n","Epoch 21/50\n","133/133 [==============================] - 65s 489ms/step - loss: 0.1163 - accuracy: 0.9678 - val_loss: 0.2516 - val_accuracy: 0.9157\n","Epoch 22/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.1100 - accuracy: 0.9692 - val_loss: 0.2718 - val_accuracy: 0.9142\n","Epoch 23/50\n","133/133 [==============================] - 66s 493ms/step - loss: 0.1077 - accuracy: 0.9737 - val_loss: 0.2423 - val_accuracy: 0.9112\n","Epoch 24/50\n","133/133 [==============================] - 66s 495ms/step - loss: 0.0997 - accuracy: 0.9734 - val_loss: 0.2399 - val_accuracy: 0.9216\n","Epoch 25/50\n","133/133 [==============================] - 66s 500ms/step - loss: 0.0926 - accuracy: 0.9760 - val_loss: 0.2290 - val_accuracy: 0.9246\n","Epoch 26/50\n","133/133 [==============================] - 67s 501ms/step - loss: 0.0916 - accuracy: 0.9770 - val_loss: 0.2329 - val_accuracy: 0.9260\n","Epoch 27/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.0862 - accuracy: 0.9779 - val_loss: 0.2375 - val_accuracy: 0.9260\n","Epoch 28/50\n","133/133 [==============================] - 65s 489ms/step - loss: 0.0886 - accuracy: 0.9781 - val_loss: 0.2447 - val_accuracy: 0.9186\n","Epoch 29/50\n","133/133 [==============================] - 66s 493ms/step - loss: 0.0888 - accuracy: 0.9767 - val_loss: 0.2357 - val_accuracy: 0.9275\n","Epoch 30/50\n","133/133 [==============================] - 67s 502ms/step - loss: 0.0793 - accuracy: 0.9821 - val_loss: 0.2296 - val_accuracy: 0.9305\n","Epoch 31/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.0793 - accuracy: 0.9817 - val_loss: 0.2744 - val_accuracy: 0.9216\n","Epoch 32/50\n","133/133 [==============================] - 66s 495ms/step - loss: 0.0769 - accuracy: 0.9805 - val_loss: 0.2209 - val_accuracy: 0.9334\n","Epoch 33/50\n","133/133 [==============================] - 65s 490ms/step - loss: 0.0797 - accuracy: 0.9812 - val_loss: 0.2483 - val_accuracy: 0.9201\n","Epoch 34/50\n","133/133 [==============================] - 66s 493ms/step - loss: 0.0639 - accuracy: 0.9861 - val_loss: 0.2521 - val_accuracy: 0.9246\n","Epoch 35/50\n","133/133 [==============================] - 66s 493ms/step - loss: 0.0630 - accuracy: 0.9873 - val_loss: 0.2624 - val_accuracy: 0.9201\n","Epoch 36/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.0669 - accuracy: 0.9850 - val_loss: 0.2276 - val_accuracy: 0.9275\n","Epoch 37/50\n","133/133 [==============================] - 65s 488ms/step - loss: 0.0641 - accuracy: 0.9878 - val_loss: 0.2476 - val_accuracy: 0.9186\n","Epoch 38/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.0628 - accuracy: 0.9866 - val_loss: 0.2617 - val_accuracy: 0.9246\n","Epoch 39/50\n","133/133 [==============================] - 65s 489ms/step - loss: 0.0564 - accuracy: 0.9885 - val_loss: 0.3267 - val_accuracy: 0.9186\n","Epoch 40/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.0546 - accuracy: 0.9908 - val_loss: 0.2561 - val_accuracy: 0.9216\n","Epoch 41/50\n","133/133 [==============================] - 65s 492ms/step - loss: 0.0529 - accuracy: 0.9892 - val_loss: 0.2502 - val_accuracy: 0.9216\n","Epoch 42/50\n","133/133 [==============================] - 65s 492ms/step - loss: 0.0527 - accuracy: 0.9906 - val_loss: 0.2477 - val_accuracy: 0.9260\n","Epoch 43/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.0596 - accuracy: 0.9875 - val_loss: 0.2521 - val_accuracy: 0.9290\n","Epoch 44/50\n","133/133 [==============================] - 65s 488ms/step - loss: 0.0539 - accuracy: 0.9901 - val_loss: 0.2466 - val_accuracy: 0.9260\n","Epoch 45/50\n","133/133 [==============================] - 65s 488ms/step - loss: 0.0450 - accuracy: 0.9939 - val_loss: 0.2850 - val_accuracy: 0.9246\n","Epoch 46/50\n","133/133 [==============================] - 65s 489ms/step - loss: 0.0498 - accuracy: 0.9904 - val_loss: 0.2771 - val_accuracy: 0.9157\n","Epoch 47/50\n","133/133 [==============================] - 66s 494ms/step - loss: 0.0519 - accuracy: 0.9899 - val_loss: 0.3291 - val_accuracy: 0.9216\n","Predictions Shape: (75, 2)\n","Accuracy: 0.9067\n","Precision: 0.9028\n","Recall: 0.9239\n","F1: 0.905\n","Done!\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/Colab Notebooks/AN2DL/\n","\n","import os\n","\n","#Fix randomness and hide warnings\n","seed = 69420\n","\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","import numpy as np\n","np.random.seed(seed)\n","\n","import logging\n","\n","import random\n","random.seed(seed)\n","\n","# Import tensorflow\n","import tensorflow as tf\n","from tensorflow import keras as tfk\n","from tensorflow.keras import layers as tfkl\n","from tensorflow.keras.applications.mobilenet import preprocess_input\n","tf.autograph.set_verbosity(0)\n","tf.get_logger().setLevel(logging.ERROR)\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)\n","#print(tf.__version__)\n","\n","# Import other libraries\n","#import cv2\n","#from tensorflow.keras.applications.mobilenet import preprocess_input\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","import seaborn as sns\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","print(\"Finished loading libraries\")\n","\n","model_name = \"model_ConNeXt_large_2_ridge_5\"\n","\n","class model:\n","    def __init__(self, path):\n","        self.model = tf.keras.models.load_model(os.path.join(path, model_name))\n","\n","    def predict(self, X):\n","\n","        # Note: this is just an example.\n","        # Here the model.predict is called, followed by the argmax\n","        out = self.model.predict(X)\n","        out = tf.argmax(out, axis=-1)  # Shape [BS]\n","\n","        return out\n","\n","\n","def show_images(images):\n","    # Show all images in in images array. Make a scrollable window if there are more than 50 images and display them in a grid\n","    num_images = len(images)    # Number of images\n","    num_cols = int(np.ceil(np.sqrt(num_images)))    # Number of columns in the grid\n","    num_rows = int(np.ceil(num_images / num_cols))\n","\n","    for i in range(num_images):\n","        plt.subplot(num_rows, num_cols, i+1)\n","        plt.imshow(images[i])\n","        plt.axis('off')\n","    plt.show()\n","\n","def plot_results(history):\n","    \"\"\" print(\"Plotting results...\")\n","    # Plot the training\n","    plt.figure(figsize=(15,5))\n","    plt.plot(history['loss'], alpha=.3, color='#ff7f0e', linestyle='--')\n","    plt.plot(history['val_loss'], label='Vanilla CNN', alpha=.8, color='#ff7f0e')\n","    plt.legend(loc='upper left')\n","    plt.title('Categorical Crossentropy')\n","    plt.grid(alpha=.3)\n","\n","    plt.figure(figsize=(15,5))\n","    plt.plot(history['accuracy'], alpha=.3, color='#ff7f0e', linestyle='--')\n","    plt.plot(history['val_accuracy'], label='Vanilla CNN', alpha=.8, color='#ff7f0e')\n","    plt.legend(loc='upper left')\n","    plt.title('Accuracy')\n","    plt.grid(alpha=.3)\n","\n","    plt.show() \"\"\"\n","\n","    # Find the epoch with the highest validation accuracy\n","    best_epoch = np.argmax(history['val_accuracy'])\n","\n","    # Plot training and validation performance metrics\n","    plt.figure(figsize=(20, 5))\n","\n","    # Plot training and validation loss\n","    plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n","    plt.plot(history['val_loss'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n","    plt.legend(loc='upper left')\n","    plt.title('Categorical Crossentropy')\n","    plt.grid(alpha=0.3)\n","\n","    plt.figure(figsize=(20, 5))\n","\n","    # Plot training and validation accuracy, highlighting the best epoch\n","    plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n","    plt.plot(history['val_accuracy'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n","    plt.plot(best_epoch, history['val_accuracy'][best_epoch], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n","    plt.legend(loc='upper left')\n","    plt.title('Accuracy')\n","    plt.grid(alpha=0.3)\n","\n","    plt.show()\n","\n","\n","def train_model():\n","    print(\"[*] Training model \", model_name, \"...\")\n","    # Load images from the .npz file\n","    data_path = 'public_data.npz'\n","    data = np.load(data_path, allow_pickle=True)\n","\n","    images = data['data']\n","    labels = data['labels']\n","\n","    \"\"\" i = 0\n","    for image in images:\n","        # Normalize image pixel values to a float range [0, 1]\n","        #images[i] = (images[i] / 255).astype(np.float32)\n","\n","        # Convert image from BGR to RGB\n","        #images[i] = images[i][...,::-1]\n","        i = i+1\n","        if (i % 1000 == 0):\n","            print(\"Processing image: \", i)\n","    print(\"Finished processing images\") \"\"\"\n","    # EfficientNetV2 models expect their inputs to be float tensors of pixels with values in the [0-255] range.\n","    images = (images).astype(np.float32)\n","\n","    # ------------------------------------------\n","    # Sanitize input\n","    # Delete trolololol and shrek\n","    positions_to_remove_old = [58, 95, 137, 138, 171, 207, 338, 412, 434, 486, 506, 529, 571,\n","                           599, 622, 658, 692, 701, 723, 725, 753, 779, 783, 827, 840, 880,\n","                           898, 901, 961, 971, 974, 989, 1028, 1044, 1064, 1065, 1101, 1149,\n","                           1172, 1190, 1191, 1265, 1268, 1280, 1333, 1384, 1443, 1466, 1483,\n","                           1528, 1541, 1554, 1594, 1609, 1630, 1651, 1690, 1697, 1752, 1757,\n","                           1759, 1806, 1828, 1866, 1903, 1938, 1939, 1977, 1981, 1988, 2022,\n","                           2081, 2090, 2150, 2191, 2192, 2198, 2261, 2311, 2328, 2348, 2380,\n","                           2426, 2435, 2451, 2453, 2487, 2496, 2515, 2564, 2581, 2593, 2596,\n","                           2663, 2665, 2675, 2676, 2727, 2734, 2736, 2755, 2779, 2796, 2800,\n","                           2830, 2831, 2839, 2864, 2866, 2889, 2913, 2929, 2937, 3033, 3049,\n","                           3055, 3086, 3105, 3108, 3144, 3155, 3286, 3376, 3410, 3436, 3451,\n","                           3488, 3490, 3572, 3583, 3666, 3688, 3700, 3740, 3770, 3800, 3801,\n","                           3802, 3806, 3811, 3821, 3835, 3862, 3885, 3896, 3899, 3904, 3927,\n","                           3931, 3946, 3950, 3964, 3988, 3989, 4049, 4055, 4097, 4100, 4118,\n","                           4144, 4150, 4282, 4310, 4314, 4316, 4368, 4411, 4475, 4476, 4503,\n","                           4507, 4557, 4605, 4618, 4694, 4719, 4735, 4740, 4766, 4779, 4837,\n","                           4848, 4857, 4860, 4883, 4897, 4903, 4907, 4927, 5048, 5080, 5082,\n","                           5121, 5143, 5165, 5171]\n","    def mse(imageA, imageB):\n","        # the 'Mean Squared Error' between the two images is the\n","        # sum of the squared difference between the two images;\n","        # NOTE: the two images must have the same dimension\n","        err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n","        err /= float(imageA.shape[0] * imageA.shape[1])\n","\n","        # return the MSE, the lower the error, the more \"similar\"\n","        # the two images are\n","        return err\n","    positions_to_remove = []\n","\n","    pos_shrek = 58\n","    pos_trolo = 338\n","    for pos in range(len(images)):\n","        if (mse(images[pos_shrek],images[pos])==0 or mse(images[pos_trolo],images[pos])==0):\n","            positions_to_remove.append(pos)\n","    if (positions_to_remove != positions_to_remove_old):\n","        print(\"ERROR: Different positions to remove\")\n","        exit()\n","    print(\"Len of positions_to_remove: \", len(positions_to_remove))\n","    n = 0\n","\n","    for pos in positions_to_remove:\n","        new_pos = pos - n\n","        #print(\"Removing image at position: \", pos, \" - New Position is \", new_pos)\n","        images = np.delete(images, new_pos, axis=0)\n","        labels = np.delete(labels, new_pos, axis=0)\n","        n = n + 1\n","\n","    # ------------------------------------------\n","\n","    labels = np.array(labels)\n","\n","    labels = LabelEncoder().fit_transform(labels)\n","    labels = tfk.utils.to_categorical(labels,len(np.unique(labels)))\n","\n","    # Use the stratify option to maintain the class distribution in the train and test datasets\n","    images_train, images_test, labels_train, labels_test = train_test_split(images, labels, test_size=0.15, stratify=np.argmax(labels, axis=1), random_state=seed)\n","\n","    # Further split the test set into test and validation sets, stratifying the labels\n","    images_test, images_val, labels_test, labels_val = train_test_split(images_test, labels_test, test_size=0.9, stratify=np.argmax(labels_test, axis=1), random_state=seed)\n","\n","    print(\"\\n\\nSHAPES OF THE SETS:\\n\")\n","\n","    print(f\"images_train shape: {images_train.shape}, labels_train shape: {labels_train.shape}\")\n","    print(f\"images_val shape: {images_val.shape}, labels_val shape: {labels_val.shape}\")\n","    print(f\"images_test shape: {images_test.shape}, labels_test shape: {labels_test.shape}\")\n","\n","    print(\"\\n\\n\")\n","\n","    input_shape = images_train.shape[1:]\n","    output_shape = labels_train.shape[1:]\n","\n","    # ------------------------------------------\n","    #Print input shape, batch size, and number of epochs\n","    #print(f\"Input Shape: {input_shape}, Output Shape: {output_shape}, Batch Size: {batch_size}, Epochs: {epochs}\")\n","    # ------------------------------------------\n","    #if include_preprocessing=True no preprocessing is needed\n","    \"\"\" efficientNet = tf.keras.applications.EfficientNetV2L(\n","        include_top=False,\n","        weights=\"imagenet\",\n","        input_shape=input_shape,\n","        pooling=None,\n","        include_preprocessing=True,\n","    ) \"\"\"\n","\n","    externalNet = tf.keras.applications.ConvNeXtLarge(\n","        model_name=\"convnext_large\",\n","        include_top=False,\n","        include_preprocessing=True,\n","        weights=\"imagenet\",\n","        #input_tensor=None,\n","        input_shape=input_shape,\n","        pooling=\"avg\",\n","        #classes=1000,\n","        #classifier_activation=\"softmax\",\n","    )\n","\n","    #Automatically get the name of the network\n","    network_keras_name = externalNet.name\n","    print(\"[*] Network name: \", network_keras_name)\n","\n","    \"\"\"mobile = tf.keras.applications.MobileNetV3Large(\n","        input_shape=None,\n","        alpha=1.0,\n","        minimalistic=False,\n","        include_top=True,\n","        weights=\"imagenet\",\n","        input_tensor=None,\n","        classes=1000,\n","        pooling=None,\n","        dropout_rate=0.2,\n","        classifier_activation=\"softmax\",\n","        include_preprocessing=True,\n","    )\"\"\"\n","\n","    for i, layer in enumerate(externalNet.layers):\n","        print(i, layer.name, layer.trainable)\n","\n","    #tfk.utils.plot_model(mobile, show_shapes=True)\n","    # Use the supernet as feature extractor, i.e. freeze all its weigths\n","    externalNet.trainable = False\n","\n","    # Create an input layer with shape (224, 224, 3)\n","    inputs = tfk.Input(shape=(96, 96, 3))\n","\n","    augmentation = tf.keras.Sequential([\n","            #tfkl.RandomBrightness(0.2, value_range=(0,1)),\n","            tfkl.RandomTranslation(0.15,0.15),\n","            #tfkl.RandomContrast(0.75),\n","            #tfkl.RandomBrightness(0.15),\n","            tfkl.RandomZoom(0.1),\n","            tfkl.RandomFlip(\"horizontal\"),\n","            tfkl.RandomFlip(\"vertical\"),\n","            tfkl.RandomRotation(0.2),\n","        ], name='preprocessing')\n","\n","    augmentation = augmentation(inputs)\n","\n","    #not needed\n","    #scale_layer = tfkl.Rescaling(scale = 1/127.5, offset = -1)\n","    #x = scale_layer(augmentation)\n","\n","    #x = mobile(augmentation)\n","    x = externalNet(augmentation)\n","\n","    \"\"\" x = tfkl.Conv2D (\n","        filters = 128,\n","        kernel_size = (3,3),\n","        activation = 'relu',\n","        name = 'Conv2D_1'\n","    ) (x) \"\"\"\n","\n","    #x = tfkl.GlobalAveragePooling2D()(x)\n","\n","    x = tfkl.Dropout(0.3)(x)\n","\n","    reg_strength = 0.03\n","    outputs = tfkl.Dense(\n","            2,\n","            kernel_regularizer=tfk.regularizers.l2(reg_strength),\n","            activation='softmax',\n","            name='Output'\n","        )(x)\n","\n","\n","    # Create a Model connecting input and output\n","    tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n","\n","    # Compile the model with Categorical Cross-Entropy loss and Adam optimizer\n","    tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n","\n","    # Display model summary\n","    tl_model.summary()\n","    # Train the model\n","    tl_history = tl_model.fit(\n","        x = images_train, # We need to apply the preprocessing thought for the MobileNetV2 network\n","        y = labels_train,\n","        batch_size = 32,\n","        epochs = 200,\n","        validation_data = (images_val, labels_val), # We need to apply the preprocessing thought for the MobileNetV2 network\n","        callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True)]\n","    ).history\n","\n","    # Save the best model\n","    tl_model.save('TransferLearningModel')\n","    del tl_model\n","\n","    # Re-load the model after transfer learning\n","    ft_model = tfk.models.load_model('TransferLearningModel')\n","    ft_model.summary()\n","\n","    # Set all MobileNetV2 layers as trainable\n","    ft_model.get_layer(network_keras_name).trainable = True\n","    #for i, layer in enumerate(ft_model.get_layer('mobilenetv2_1.00_96').layers):\n","    #    print(i, layer.name, layer.trainable)\n","\n","    # Freeze first N layers, e.g., until the 133rd one\n","    #N = 270\n","    N = 125\n","    for i, layer in enumerate(ft_model.get_layer(network_keras_name).layers[:N]):\n","        layer.trainable=False\n","    for i, layer in enumerate(ft_model.get_layer(network_keras_name).layers):\n","        print(i, layer.name, layer.trainable)\n","    ft_model.summary()\n","\n","    # Compile the model\n","    ft_model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(1e-5), metrics='accuracy')\n","\n","    # Fine-tune the model\n","    ft_history = ft_model.fit(\n","        x = images_train, # We need to apply the preprocessing thought for the MobileNetV2 network\n","        y = labels_train,\n","        batch_size = 32,\n","        epochs = 50,\n","        validation_data = (images_val, labels_val), # We need to apply the preprocessing thought for the MobileNetV2 network\n","        callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=15, restore_best_weights=True)]\n","    ).history\n","\n","    # Save the model\n","    ft_model.save(model_name)\n","\n","    # ------------------------------------------\n","    plot_result = False\n","    if plot_result:\n","        plot_results(ft_history)\n","    # ------------------------------------------\n","\n","    evaluate = True\n","    if evaluate:\n","        # Evaluate the model on the test set\n","        # Copilot generated version:\n","        # test_loss, test_acc = model.evaluate(images_test, labels_test, verbose=2)\n","\n","        # Notebooks version:\n","        # Predict labels for the entire test set\n","        predictions = ft_model.predict(images_test, verbose=0)\n","\n","        # Display the shape of the predictions\n","        print(\"Predictions Shape:\", predictions.shape)\n","\n","        # Compute the confusion matrix\n","        cm = confusion_matrix(np.argmax(labels_test, axis=-1), np.argmax(predictions, axis=-1))\n","\n","        # Compute classification metrics\n","        accuracy = accuracy_score(np.argmax(labels_test, axis=-1), np.argmax(predictions, axis=-1))\n","        precision = precision_score(np.argmax(labels_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n","        recall = recall_score(np.argmax(labels_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n","        f1 = f1_score(np.argmax(labels_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n","\n","        # Display the computed metrics\n","        print('Accuracy:', accuracy.round(4))\n","        print('Precision:', precision.round(4))\n","        print('Recall:', recall.round(4))\n","        print('F1:', f1.round(4))\n","\n","        #print(\"\\n0:Healthy, 1:Unhealthy\\n\")\n","        # Plot the confusion matrix\n","        \"\"\" plt.figure(figsize=(10, 8))\n","        sns.heatmap(cm.T, annot=True, xticklabels=np.unique(labels_test), yticklabels=np.unique(labels_test), cmap='Blues')\n","        plt.xlabel('True labels')\n","        plt.ylabel('Predicted labels')\n","        plt.show() \"\"\"\n","\n","# ------------------------------------------\n","if __name__ == \"__main__\":\n","    train = True\n","    if (train):\n","        train_model()\n","    else:\n","        print(\"No training :(\")\n","    #_model = model(os.getcwd())\n","\n","    \"\"\" # Load images from the .npz file\n","    __data_path = 'Challenge 1/data/phase_1/public_data.npz'\n","    __data = np.load(__data_path, allow_pickle=True)\n","\n","    __images = __data['data']\n","    __labels = __data['labels']\n","\n","    i = 0\n","    for __image in __images:\n","        # Normalize image pixel values to a float range [0, 1]\n","        __images[i] = (__images[i] / 255).astype(np.float32)\n","        # Convert image from BGR to RGB\n","        __images[i] = __images[i][...,::-1]\n","        if (i % 100 == 0):\n","            print(\"Processing image: \", i, \"\\n\")\n","            #pred = _model.predict(__image)\n","            #print(\"Prediction: \", pred, \" - Label: \", __labels[i])\n","        i = i+1\n","\n","    # print the shape of __images\n","    print(__images.shape)\n","\n","    pred = _model.predict(__images)\n","    print(pred)\n","\n","    for y in pred:\n","        print(y, \"\\n\") \"\"\"\n","\n","    print(\"Done!\")"]}]}