{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Graph viewer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "# Fix randomness and hide warnings\n",
    "seed = 42\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import logging\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "print(tf.__version__)\n",
    "\n",
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of positions_to_remove:  196\n"
     ]
    }
   ],
   "source": [
    "data_path = 'public_data.npz'\n",
    "data = np.load(data_path, allow_pickle=True)\n",
    "\n",
    "images = data['data']\n",
    "labels = data['labels']\n",
    "\n",
    "# EfficientNetV2 models expect their inputs to be float tensors of pixels with values in the [0-255] range.\n",
    "images = (images).astype(np.float32)\n",
    "\n",
    "# ------------------------------------------\n",
    "# Sanitize input\n",
    "# Delete trolololol and shrek\n",
    "positions_to_remove_old = [58, 95, 137, 138, 171, 207, 338, 412, 434, 486, 506, 529, 571, \n",
    "                        599, 622, 658, 692, 701, 723, 725, 753, 779, 783, 827, 840, 880, \n",
    "                        898, 901, 961, 971, 974, 989, 1028, 1044, 1064, 1065, 1101, 1149, \n",
    "                        1172, 1190, 1191, 1265, 1268, 1280, 1333, 1384, 1443, 1466, 1483, \n",
    "                        1528, 1541, 1554, 1594, 1609, 1630, 1651, 1690, 1697, 1752, 1757,\n",
    "                        1759, 1806, 1828, 1866, 1903, 1938, 1939, 1977, 1981, 1988, 2022, \n",
    "                        2081, 2090, 2150, 2191, 2192, 2198, 2261, 2311, 2328, 2348, 2380, \n",
    "                        2426, 2435, 2451, 2453, 2487, 2496, 2515, 2564, 2581, 2593, 2596, \n",
    "                        2663, 2665, 2675, 2676, 2727, 2734, 2736, 2755, 2779, 2796, 2800, \n",
    "                        2830, 2831, 2839, 2864, 2866, 2889, 2913, 2929, 2937, 3033, 3049, \n",
    "                        3055, 3086, 3105, 3108, 3144, 3155, 3286, 3376, 3410, 3436, 3451,\n",
    "                        3488, 3490, 3572, 3583, 3666, 3688, 3700, 3740, 3770, 3800, 3801, \n",
    "                        3802, 3806, 3811, 3821, 3835, 3862, 3885, 3896, 3899, 3904, 3927, \n",
    "                        3931, 3946, 3950, 3964, 3988, 3989, 4049, 4055, 4097, 4100, 4118, \n",
    "                        4144, 4150, 4282, 4310, 4314, 4316, 4368, 4411, 4475, 4476, 4503,\n",
    "                        4507, 4557, 4605, 4618, 4694, 4719, 4735, 4740, 4766, 4779, 4837,\n",
    "                        4848, 4857, 4860, 4883, 4897, 4903, 4907, 4927, 5048, 5080, 5082, \n",
    "                        5121, 5143, 5165, 5171]\n",
    "def mse(imageA, imageB):\n",
    "    # the 'Mean Squared Error' between the two images is the\n",
    "    # sum of the squared difference between the two images;\n",
    "    # NOTE: the two images must have the same dimension\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    \n",
    "    # return the MSE, the lower the error, the more \"similar\"\n",
    "    # the two images are\n",
    "    return err\n",
    "positions_to_remove = []\n",
    "\n",
    "pos_shrek = 58\n",
    "pos_trolo = 338\n",
    "for pos in range(len(images)):\n",
    "    if (mse(images[pos_shrek],images[pos])==0 or mse(images[pos_trolo],images[pos])==0):\n",
    "        positions_to_remove.append(pos)\n",
    "if (positions_to_remove != positions_to_remove_old):\n",
    "    print(\"ERROR: Different positions to remove\")\n",
    "    exit()\n",
    "print(\"Len of positions_to_remove: \", len(positions_to_remove))\n",
    "n = 0\n",
    "\n",
    "for pos in positions_to_remove:\n",
    "    new_pos = pos - n\n",
    "    #print(\"Removing image at position: \", pos, \" - New Position is \", new_pos)\n",
    "    images = np.delete(images, new_pos, axis=0)\n",
    "    labels = np.delete(labels, new_pos, axis=0)\n",
    "    n = n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/siro/GitHub/AN2DL-Challenges-24/Challenge 1/code/graphs_viewer.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siro/GitHub/AN2DL-Challenges-24/Challenge%201/code/graphs_viewer.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(labels)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/siro/GitHub/AN2DL-Challenges-24/Challenge%201/code/graphs_viewer.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m labels \u001b[39m=\u001b[39m LabelEncoder()\u001b[39m.\u001b[39mfit_transform(labels)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siro/GitHub/AN2DL-Challenges-24/Challenge%201/code/graphs_viewer.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m labels \u001b[39m=\u001b[39m tfk\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mto_categorical(labels,\u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(labels)))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/siro/GitHub/AN2DL-Challenges-24/Challenge%201/code/graphs_viewer.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Use the stratify option to maintain the class distribution in the train and test datasets\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "labels = np.array(labels)\n",
    "\n",
    "labels = LabelEncoder().fit_transform(labels)\n",
    "labels = tfk.utils.to_categorical(labels,len(np.unique(labels)))\n",
    "\n",
    "# Use the stratify option to maintain the class distribution in the train and test datasets\n",
    "images_train, images_val, labels_train, labels_val = train_test_split(images, labels, test_size=0.20, stratify=np.argmax(labels, axis=1), random_state=seed)\n",
    "\n",
    "# Further split the test set into test and validation sets, stratifying the labels\n",
    "#images_test, images_val, labels_test, labels_val = train_test_split(images_test, labels_test, test_size=0.9, stratify=np.argmax(labels_test, axis=1), random_state=seed)\n",
    "\n",
    "print(\"\\n\\nSHAPES OF THE SETS:\\n\")\n",
    "\n",
    "print(f\"images_train shape: {images_train.shape}, labels_train shape: {labels_train.shape}\")\n",
    "print(f\"images_val shape: {images_val.shape}, labels_val shape: {labels_val.shape}\")\n",
    "#print(f\"images_test shape: {images_test.shape}, labels_test shape: {labels_test.shape}\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "input_shape = images_train.shape[1:]\n",
    "output_shape = labels_train.shape[1:]\n",
    "\n",
    "#Print input shape, batch size, and number of epochs\n",
    "#print(f\"Input Shape: {input_shape}, Output Shape: {output_shape}, Batch Size: {batch_size}, Epochs: {epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(history):\n",
    "    # Find the epoch with the highest validation accuracy\n",
    "    best_epoch = np.argmax(history['val_accuracy'])\n",
    "\n",
    "    # Plot training and validation performance metrics\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "    plt.plot(history['val_loss'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('Categorical Crossentropy')\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    # Plot training and validation accuracy, highlighting the best epoch\n",
    "    plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=3)\n",
    "    plt.plot(history['val_accuracy'], label='Validation', alpha=0.8, color='#4D61E2', linewidth=3)\n",
    "    plt.plot(best_epoch, history['val_accuracy'][best_epoch], marker='*', alpha=0.8, markersize=10, color='#4D61E2')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('Accuracy')\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model_name = \"MODEL NAME\"\n",
    "    externalNet = tf.keras.applications.ConvNeXtLarge(\n",
    "        model_name=\"convnext_large\",\n",
    "        include_top=False,\n",
    "        include_preprocessing=True,\n",
    "        weights=\"imagenet\",\n",
    "        #input_tensor=None,\n",
    "        input_shape=input_shape,\n",
    "        pooling=None,\n",
    "        #classes=1000,\n",
    "        #classifier_activation=\"softmax\",\n",
    "    )\n",
    "\n",
    "    #Automatically get the name of the network\n",
    "    network_keras_name = externalNet.name\n",
    "    print(\"[*] Network name: \", network_keras_name)\n",
    "\n",
    "    #tfk.utils.plot_model(mobile, show_shapes=True)\n",
    "    # Use the supernet as feature extractor, i.e. freeze all its weigths\n",
    "    externalNet.trainable = False\n",
    "\n",
    "    # Create an input layer with shape (224, 224, 3)\n",
    "    inputs = tfk.Input(shape=(96, 96, 3))\n",
    "\n",
    "    augmentation = tf.keras.Sequential([\n",
    "            #tfkl.RandomBrightness(0.2, value_range=(0,1)),\n",
    "            tfkl.RandomTranslation(0.15,0.15),\n",
    "            #tfkl.RandomContrast(0.75),\n",
    "            tfkl.RandomBrightness(0.15),\n",
    "            tfkl.RandomZoom(0.1),\n",
    "            tfkl.RandomFlip(\"horizontal\"),\n",
    "            tfkl.RandomFlip(\"vertical\"),\n",
    "            tfkl.RandomRotation(0.2),\n",
    "        ], name='preprocessing')\n",
    "\n",
    "    augmentation = augmentation(inputs)\n",
    "\n",
    "    x = externalNet(augmentation)\n",
    "\n",
    "    x = tfkl.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = tfkl.Dropout(0.2)(x)\n",
    "\n",
    "    reg_strength = 0.05\n",
    "    outputs = tfkl.Dense(\n",
    "            2,\n",
    "            kernel_regularizer=tfk.regularizers.l2(reg_strength),\n",
    "            activation='softmax',\n",
    "            name='Output'\n",
    "        )(x)\n",
    "\n",
    "    # Create a Model connecting input and output\n",
    "    tl_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
    "\n",
    "    # Compile the model with Categorical Cross-Entropy loss and Adam optimizer\n",
    "    tl_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "    # Display model summary\n",
    "    tl_model.summary()\n",
    "    # Train the model\n",
    "    tl_history = tl_model.fit(\n",
    "        x = images_train, # We need to apply the preprocessing thought for the MobileNetV2 network\n",
    "        y = labels_train,\n",
    "        batch_size = 32,\n",
    "        epochs = 500,\n",
    "        validation_data = (images_val, labels_val), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
    "        callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=100, restore_best_weights=True)]\n",
    "    ).history\n",
    "\n",
    "    # Save the best model\n",
    "    tl_model.save('TransferLearningModel')\n",
    "    del tl_model\n",
    "\n",
    "    # Re-load the model after transfer learning\n",
    "    ft_model = tfk.models.load_model('TransferLearningModel')\n",
    "    ft_model.summary()\n",
    "\n",
    "    # Set all MobileNetV2 layers as trainable\n",
    "    ft_model.get_layer(network_keras_name).trainable = True\n",
    "    #for i, layer in enumerate(ft_model.get_layer('mobilenetv2_1.00_96').layers):\n",
    "    #    print(i, layer.name, layer.trainable)\n",
    "\n",
    "    # Freeze first N layers\n",
    "    #N = 270\n",
    "    N = 125\n",
    "    for i, layer in enumerate(ft_model.get_layer(network_keras_name).layers[:N]):\n",
    "        layer.trainable=False\n",
    "    \"\"\" for i, layer in enumerate(ft_model.get_layer(network_keras_name).layers):\n",
    "        print(i, layer.name, layer.trainable) \"\"\"\n",
    "    ft_model.summary()\n",
    "\n",
    "    # Compile the model\n",
    "    ft_model.compile(loss=tfk.losses.BinaryCrossentropy(), optimizer=tfk.optimizers.Adam(1e-5), metrics='accuracy')\n",
    "\n",
    "    # Fine-tune the model\n",
    "    ft_history = ft_model.fit(\n",
    "        x = images_train, # We need to apply the preprocessing thought for the MobileNetV2 network\n",
    "        y = labels_train,\n",
    "        batch_size = 32,\n",
    "        epochs = 500,\n",
    "        validation_data = (images_val, labels_val), # We need to apply the preprocessing thought for the MobileNetV2 network\n",
    "        callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=100, restore_best_weights=True)]\n",
    "    ).history\n",
    "\n",
    "    plot_results(ft_history)\n",
    "    \n",
    "    # Save the model\n",
    "    ft_model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data():\n",
    "    # Load the model\n",
    "    model_to_load = \"MODEL NAME\"\n",
    "    model = tfk.models.load_model(model_to_load)\n",
    "    model.summary()\n",
    "\n",
    "    image = images[0]\n",
    "    # Extract activations from the first convolutional layer for the noise image\n",
    "    first_conv = tfk.Sequential(model.layers[:2])\n",
    "    first_activations = first_conv(image)\n",
    "\n",
    "    # Extract activations from the second convolutional layer for the noise image\n",
    "    second_conv = tfk.Sequential(model.layers[:4])\n",
    "    second_activations = second_conv(image)\n",
    "\n",
    "    # Display the input image, true label, and predicted label\n",
    "    print('Input image')\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 5)\n",
    "    ax1.imshow(np.squeeze(image), cmap='gray')\n",
    "    prediction = model.predict(image, verbose=0)\n",
    "    ax2.barh(list(labels.values()), np.squeeze(prediction, axis=0), color=plt.get_cmap('Paired').colors)\n",
    "    ax2.set_title('Predicted label: ' + labels[np.argmax(prediction)])\n",
    "    ax2.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # Display activations from the first convolutional layer\n",
    "    print('First convolutional activations')\n",
    "    num_row = 1\n",
    "    num_col = 6\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(14, 12))\n",
    "    for i in range(num_row * num_col):\n",
    "        ax = axes[i % num_col]\n",
    "        ax.imshow(first_activations[0,:,:,i], cmap='gray')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Display activations from the second convolutional layer\n",
    "    print('Second convolutional activations')\n",
    "    num_row = 2\n",
    "    num_col = 8\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(14, 4))\n",
    "    for i in range(num_row * num_col):\n",
    "        ax = axes[i // num_col, i % num_col]\n",
    "        ax.imshow(second_activations[0,:,:,i], cmap='gray')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Define images_test and labels_test\n",
    "    images_test = images_val\n",
    "    labels_test = labels_val\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    predictions = model_to_load.predict(images_test, verbose=0)\n",
    "\n",
    "    # Display the shape of the predictions\n",
    "    print(\"Predictions Shape:\", predictions.shape)\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(np.argmax(labels_test, axis=-1), np.argmax(predictions, axis=-1))\n",
    "\n",
    "    # Compute classification metrics\n",
    "    accuracy = accuracy_score(np.argmax(labels_test, axis=-1), np.argmax(predictions, axis=-1))\n",
    "    precision = precision_score(np.argmax(labels_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "    recall = recall_score(np.argmax(labels_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "    f1 = f1_score(np.argmax(labels_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n",
    "\n",
    "    # Display the computed metrics\n",
    "    print('Accuracy:', accuracy.round(4))\n",
    "    print('Precision:', precision.round(4))\n",
    "    print('Recall:', recall.round(4))\n",
    "    print('F1:', f1.round(4))\n",
    "\n",
    "    print(\"\\n0:Healthy, 1:Unhealthy\\n\")\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm.T, annot=True, xticklabels=np.unique(labels_test), yticklabels=np.unique(labels_test), cmap='Blues')\n",
    "    plt.xlabel('True labels')\n",
    "    plt.ylabel('Predicted labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = False\n",
    "\n",
    "if training:\n",
    "    train_model()\n",
    "    #Also shows the results of the training\n",
    "else:\n",
    "    test_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
